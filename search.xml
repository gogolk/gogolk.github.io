<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[PriceCompare]]></title>
    <url>%2F2018%2F09%2F11%2FPriceCompare%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;通过Pyhton编写爬虫实时爬取各大数字货币交易平台(火币网，币安网等等)的比特币和以太币等等价格，并做价格比对。 &emsp;&emsp;数据爬取一开始做的版本是设置一个间隔时间interval，每隔interval就向各个发送请求获得response再到response中解析所需，缺点是每次发送请求到获得response所需时间较长。分析各个交易网站后，他们更新数据都是用的websocket进行同行，所以果断改写通过websocket连接，实时接收服务器发送过来的信息，存入redis库。然后通过一个子进程获得所有最近的价格，另实现断开自动重连。注意：这些交易平台，其中www.bitfinex.com是需要挂代理的，详情见代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234#PriceSpiders#将各个网站的价格信息实时存入redis中class PriceSpiders(object): #火币网 https://www.hbg.com/zh-cn #币夫网 https://www.bitforex.com/ #币安网 https://www.binance.co/?ref=11295221 #fcoin https://www.fcoin.com/ #abcc https://abcc.com/ #vpn #bitfinex https://www.bitfinex.com/ #vpn def __init__(self): self.headers = &#123;'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36'&#125; self.proxies = &#123;'http':'127.0.0.1:1080', 'https':'127.0.0.1:1080'&#125; def get_bitforex_price(self,coin): #币夫网 https://www.bitforex.com/ r = redis.Redis(connection_pool=redis.ConnectionPool(host='127.0.0.1', port=6379, db=1)) if coin == 'b': coin = 'btcusdt' elif coin == 'e': coin = 'ethusdt' else: raise ValueError('coin only b or e') def on_open(ws): ws.send('[&#123;"type":"subHq_cancel_all","event":"kline"&#125;]') time.sleep(0.2) ws.send('[&#123;"type":"subHq","event":"kline","param":&#123;"businessType":"coin-usdt-%s","kType":"30min","size":1440&#125;&#125;]' % coin[:3]) def on_close(ws): if coin == 'btcusdt': self.get_bitforex_price('b') else: self.get_bitforex_price('e') def on_message(ws, massage): if len(massage) &lt; 1000: data = json.loads(massage) price = data['data'][0]['close'] r.lpush('Prices:bitforex:&#123;&#125;'.format(coin.lower()), '&#123;:.2f&#125;'.format(float(price))) r.expire('Prices:bitforex:&#123;&#125;'.format(coin.lower()), 300) def on_ping(ws): while True: time.sleep(10) ws.send('ping_p') app = websocket.WebSocketApp('wss://wscn.bitforex.com/mkapi/coinGroup1/ws', header=self.headers, on_open=on_open, on_message=on_message, on_close=on_close, ) t = threading.Thread(target=on_ping, args=(app,)) t.daemon = True t.start() app.run_forever() def get_hbg_price(self,coin): #火币网 https://www.hbg.com/zh-cn r = redis.Redis(connection_pool=redis.ConnectionPool(host='127.0.0.1', port=6379, db=1)) if coin == 'b': coin = 'btcusdt' elif coin == 'e': coin = 'ethusdt' else: raise ValueError('coin only b or e') def on_open(ws): ws.send('&#123;"sub":"market.%s.detail"&#125;' %coin) time.sleep(0.2) ws.send('[&#123;"type":"subHq","event":"kline","param":&#123;"businessType":"coin-usdt-%s","kType":"30min","size":1440&#125;&#125;]' % coin[:3]) #ws.send('[&#123;"type": "subHq", "event": "trade", "param": &#123;"businessType": "coin-usdt-eth", "dType":0, "size": 100&#125;&#125;]') def on_close(ws): if coin == 'btcusdt': self.get_hbg_price('b') else: self.get_hbg_price('e') def on_message(ws, massage): massage = bytes.decode(zlib.decompress(massage,16+zlib.MAX_WBITS),encoding='utf8') if json.loads(massage).__contains__('ping'): ws.send('"pong":%s' % datetime.datetime.now().timestamp()) else: price = json.loads(massage)['tick']['close'] r.lpush('Prices:hbg:&#123;&#125;'.format(coin.lower()), '&#123;:.2f&#125;'.format(float(price))) r.expire('Prices:hgb:&#123;&#125;'.format(coin.lower()), 300) app = websocket.WebSocketApp('wss://www.hbg.com/-/s/pro/ws', header=self.headers, on_open=on_open, on_message=on_message, on_close=on_close ) app.run_forever() def get_binance_price(self, coin): #币安网 https://www.binance.co/?ref=11295221 r = redis.Redis(connection_pool=redis.ConnectionPool(host='127.0.0.1', port=6379, db=1)) if coin == 'b': coin = 'BTCUSDT' elif coin == 'e': coin = 'ETHUSDT' else: raise ValueError('coin only b or e') def on_close(ws): if coin == 'btcusdt': self.get_biance_price('b') else: self.get_biance_price('e') def on_message(ws, message): data = (json.loads(message)['data']) for k, i in enumerate(data): if i['s'] == coin: price = i['c'] r.lpush('Prices:binance:&#123;&#125;'.format(coin.lower()), '&#123;:.2f&#125;'.format(float(price))) r.expire('Prices:binance:&#123;&#125;'.format(coin.lower()), 300) app = websocket.WebSocketApp("wss://stream2.binance.cloud/stream?streams=!miniTicker@arr@3000ms", header=self.headers, on_message=on_message) app.run_forever() def get_fcoin_price(self,coin): # fcoin https://www.fcoin.com/ r = redis.Redis(connection_pool=redis.ConnectionPool(host='127.0.0.1', port=6379, db=1)) if coin == 'b': coin = 'btcusdt' elif coin == 'e': coin = 'ethusdt' else: raise ValueError def on_close(ws): if coin == 'btcusdt': self.get_fcoin_price('b') else: self.get_fcoin_price('e') def on_message(ws, message): datas = json.loads(message) if datas.__contains__('tickers'): datas = datas['tickers'] for k, i in enumerate(datas): if i['symbol'] == coin: price = (i['ticker'][0]) r.lpush('Prices:fcoin:&#123;&#125;'.format(coin.lower()), '&#123;:.2f&#125;'.format(float(price))) r.expire('Prices:fcoin:&#123;&#125;'.format(coin.lower()), 300) def on_open(ws): ws.send('&#123;"id": "tickers", "cmd": "sub", "args": ["all-tickers"]&#125;') websocket.enableTrace = True app = websocket.WebSocketApp("wss://ws.fcoin.com/api/v2/ws", header=self.headers, on_open=on_open, on_message=on_message, on_close=on_close ) app.run_forever() def get_abcc_price(self, coin): # abcc https://abcc.com/ #vpn r = redis.Redis(connection_pool=redis.ConnectionPool(host='127.0.0.1', port=6379, db=1)) if coin == 'b': coin = 'btcusdt' elif coin == 'e': coin = 'ethusdt' else: raise ValueError def on_close(ws): if coin == 'btcusdt': self.get_bitforex_price('b') else: self.get_bitforex_price('e') def on_message(ws, message): if len(message) &gt; 30000: price = json.loads(json.loads(message)['data'])[coin]['last'] r.lpush('Prices:abcc:&#123;&#125;'.format(coin.lower()), '&#123;:.2f&#125;'.format(float(price))) r.expire('Prices:abcc:&#123;&#125;'.format(coin.lower()), 300) def on_open(ws): ws.send('&#123;"event":"pusher:subscribe","data":&#123;"channel":"market-global"&#125;&#125;') websocket.enableTrace = True app = websocket.WebSocketApp("wss://push.abcc.com/app/2d1974bfdde17e8ecd3e7f0f6e39816b?protocol=7&amp;client=js&amp;version=4.2.2&amp;flash=false", header=self.headers, on_open=on_open, on_message=on_message, on_close=on_close ) app.run_forever() def get_bitfinex_price(self,coin): # bitfinex https://www.bitfinex.com/ #vpn #该网站不是通过ws更新数据的，而是循环请求。 r = redis.Redis(connection_pool=redis.ConnectionPool(host='127.0.0.1', port=6379, db=1)) if coin == 'b': coin = 'btcusdt' elif coin == 'e': coin = 'ethusdt' else: raise ValueError while True: url = 'https://api.bitfinex.com/v2/tickers?symbols=ALL' res = requests.get(url, headers=self.headers, proxies=self.proxies).content if coin == 'btcusdt': price = json.loads((bytes.decode(res)))[0][1] r.lpush('Prices:bitfinex:&#123;&#125;'.format(coin.lower()), '&#123;:.2f&#125;'.format(float(price))) r.expire('Prices:bitfinex:&#123;&#125;'.format(coin.lower()), 300) else: price = json.loads(bytes.decode(res))[3][1] r.lpush('Prices:bitfinex:&#123;&#125;'.format(coin.lower()), '&#123;:.2f&#125;'.format(float(price))) r.expire('Prices:bitfinex:&#123;&#125;'.format(coin.lower()), 300) time.sleep(5) def all_spiders(self): #返回所有spider名称的list return filter(lambda x: x.startswith('get_'), dir(self))def main(): spider_list = list(PriceSpiders().all_spiders())[:] with ThreadPoolExecutor(max_workers=20) as executor: for spider in spider_list: executor.submit(eval('PriceSpiders().%s' %spider), 'b') executor.submit(eval('PriceSpiders().%s' %spider), 'e')if __name__ == '__main__': main() &emsp;&emsp;最开始到这里再写个获取redis中的最新价格就可以结束了的，但是又想自己写个websocket server把最新价格实时发送到网站上去。所以，通过socket监听端口，获得请求后创建一个子线程连接websocket。当然，最好是用上次使用的协程，这样占用资源更少，更适合高并发。 首先是获取最新prices，每两秒更新一次最新的价格，12345678910111213def get_prices(prices): r = redis.Redis(connection_pool=redis.ConnectionPool(host='127.0.0.1', port='6379', db=1)) spider_list = list(PriceSpiders().all_spiders())[:] while True: for spider in spider_list: try: price = r.lindex('Prices:%s:btcusdt'% spider.split('_')[1], 1) prices['btcusdt'][spider.split('_')[1]] = bytes.decode(price, encoding='utf8') price = r.lindex('Prices:%s:ethusdt'% spider.split('_')[1], 1) prices['ethusdt'][spider.split('_')[1]] = bytes.decode(price, encoding='utf8') except: pass time.sleep(2) &emsp;&emsp;接下来编写处理请求的线程,分为两块，一块是握手，实现websocket连接，第二快sendmassage，将需要发送的信息处理成websocket协议信息格式。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#websocket serverimport socketimport timeimport hashlibimport base64import structimport redisimport jsonfrom multiprocessing import Process, Managerfrom PricesCompare2 import PriceSpidersfrom threading import Threadclass returnCrossDomain(Thread): def __init__(self, connection,prices): Thread.__init__(self) self.con = connection self.isHandleShake = False self.prices = prices def run(self): while True: if not self.isHandleShake: # 握手 clientData =self.con.recv(1024) print(clientData) dataList = clientData.split(b"\r\n") header = &#123;&#125; for data in dataList: if b": " in data: unit = data.split(b": ") header[unit[0]] = unit[1] secKey = header[b'Sec-WebSocket-Key'] resKey = base64.encodebytes(hashlib.new("sha1", secKey + b"258EAFA5-E914-47DA-95CA-C5AB0DC85B11").digest()).replace(b'\n', b'') response = b'''HTTP/1.1 101 Switching Protocols\r\n''' response += b'''Upgrade: websocket\r\n''' response += b'''Connection: Upgrade\r\n''' response += b'''Sec-WebSocket-Accept: %s\r\n\r\n''' % (resKey,) self.con.send(response) self.isHandleShake = True else: prices = &#123;coin:&#123;source:price for source,price in self.prices[coin].items()&#125; for coin in self.prices.keys()&#125; self.sendMessage(json.dumps(prices)) time.sleep(2) # 发送websocket server报文部分 def sendMessage(self, message): msgLen = len(message) backMsgList = [] backMsgList.append(struct.pack('B', 129)) if msgLen &lt;= 125: backMsgList.append(struct.pack('b', msgLen)) elif msgLen &lt;= 65535: backMsgList.append(struct.pack('b', 126)) backMsgList.append(struct.pack('&gt;h', msgLen)) elif msgLen &lt;= (2 ** 64 - 1): backMsgList.append(struct.pack('b', 127)) backMsgList.append(struct.pack('&gt;q', msgLen)) else: print("the message is too long to send in a time") return message_byte = bytes() for c in backMsgList: message_byte += c message_byte += bytes(message, encoding="utf8") self.con.send(message_byte)def server(prices): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.bind(('127.0.0.1', 9001)) sock.listen(100) while True: try: connection, address = sock.accept() returnCrossDomain(connection, prices).start() except: time.sleep(1) &emsp;&emsp;两个进程，一个进程更新prices，另一个进程像客户端发送最新的prices，所以需要进程间共享变量，这里使用的是multiprocess.Manager,因为它可以很好的支持dict。123456789101112131415#main()def main() manager = Manager() prices = manager.dict(&#123;'btcusdt': manager.dict(&#123;&#125;), 'ethusdt': manager.dict(&#123;&#125;)&#125;) plist = [] p1 = Process(target=get_prices, args=(prices,)) plist.append(p1) p2 = Process(target=server, args=(prices,)) plist.append(p2) for p in plist: p.daemon = True p.start() for p in plist: p.join() &emsp;&emsp;接下来是客户端，建立一个表格，与服务器建立websocket连接，实时接收信息并更新网页，并显示最高值和最低值。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;meta name='author' content="lk"&gt; &lt;title&gt;PriceCompare&lt;/title&gt; &lt;link rel="stylesheet" href="style.css"&gt; &lt;script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8.0/jquery.min.js"&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;h2&gt;PriceCompare&lt;/h2&gt; &lt;table class="win" &gt; &lt;tr class="sources"&gt; &lt;td&gt;&lt;/td&gt; &lt;td class="source"&gt;abcc&lt;/td&gt; &lt;td class="source"&gt;binance&lt;/td&gt; &lt;td class="source"&gt;bitfinex&lt;/td&gt; &lt;td class="source"&gt;bitforex&lt;/td&gt; &lt;td class="source"&gt;fcoin&lt;/td&gt; &lt;td class="source"&gt;hbg&lt;/td&gt; &lt;/tr&gt; &lt;tr class="coins"&gt; &lt;td class="coin"&gt;btcusdt&lt;/td&gt; &lt;td class="btcusdt-abcc"&gt;&lt;/td&gt; &lt;td class="btcusdt-binance"&gt;&lt;/td&gt; &lt;td class="btcusdt-bitfinex"&gt;&lt;/td&gt; &lt;td class="btcusdt-bitforex"&gt;&lt;/td&gt; &lt;td class="btcusdt-fcoin"&gt;&lt;/td&gt; &lt;td class="btcusdt-hbg"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr class="coins"&gt; &lt;td class="coin"&gt;ethusdt&lt;/td&gt; &lt;td class="ethusdt-abcc"&gt;&lt;/td&gt; &lt;td class="ethusdt-binance"&gt;&lt;/td&gt; &lt;td class="ethusdt-bitfinex"&gt;&lt;/td&gt; &lt;td class="ethusdt-bitforex"&gt;&lt;/td&gt; &lt;td class="ethusdt-fcoin"&gt;&lt;/td&gt; &lt;td class="ethusdt-hbg"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;button onclick="start()"&gt;start&lt;/button&gt; &lt;script type="text/javascript"&gt; function start() &#123; let ws; ws = new WebSocket('ws://localhost:9001'); ws.onmessage = function(message) &#123; $('td').css("color","white"); prices = JSON.parse(message['data']) for (let coin in prices) &#123; let max_ = 0; let min_ = 100000; for (price in prices[coin]) &#123;; if (prices[coin][price] &lt; min_) &#123; min_ = prices[coin][price]; min_name = price &#125; else if (prices[coin][price] &gt; max_) &#123; max_ = prices[coin][price]; max_name = price &#125; $(`.$&#123;coin&#125;-$&#123;price&#125;`).text(prices[coin][price]) &#125; $(`.$&#123;coin&#125;-$&#123;max_name&#125;`).css("color","red"); $(`.$&#123;coin&#125;-$&#123;min_name&#125;`).css('color', 'green'); &#125; &#125; &#125; &lt;/script&gt; &lt;/body&gt;&lt;/html&gt;gif演示 界面丑请忽略吧.. 红色是现在该货币最高价格，绿色为最低价格。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python开发：文献下载管理软件DocManger]]></title>
    <url>%2F2018%2F09%2F01%2F2018-09-1-Docmanager%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;绝大多数文献网站都需要权限才能进行文献下载，高校统一购买了权限，让师生在校内在校园网上免费使用。但是这就意味着学校之外想查阅文献无法立即查阅下载。通过scrapy-redis，把文献相关信息整理入库，并分类将文献下载到本地，实现一个本地文献数据库。&emsp;&emsp;1. 项目背景。&emsp;&emsp;2. 工具介绍。&emsp;&emsp;3. scrapy爬取文献信息集合入redis。&emsp;&emsp;4. 将redis中文献集合去重本地已有文献，只留下新文献集合。&emsp;&emsp;5. 爬取新文献详细信息。&emsp;&emsp;6. 新文献下载。&emsp;&emsp;7. 总结。 项目背景&emsp;&emsp;校外无法下载查阅的文献，通过提前将文献批量下载到本地，如果有需要，在本地查询即可。并且所得信息可以获取更多有意思的数据。 工具介绍ScrapyScrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。 其主要分为5个部分，engine，download，scheduler，spider，pipeline。engine是调度整个爬虫框架其余部件的核心。scheduler管理request，scheduler发送request给download去下载，然后获得response给spider，spider解析网页将新的request给scheduler，将数据item发给pipeline去处理，pipeline做数据处理，存储等工作。 RedisREmote DIctionary Server(Redis) 是一个由Salvatore Sanfilippo写的key-value存储系统。 Redis是一个开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。 它通常被称为数据结构服务器，因为值（value）可以是 字符串(String), 哈希(Map), 列表(list), 集合(sets) 和 有序集合(sorted sets)等类型。 MysqlMySQL 是最流行的关系型数据库管理系统，在WEB应用方面 MySQL 是最好的RDBMS(Relational Database Management System：关系数据库管理系统)应用软件之一。 scrapy爬取文献信息集合入redis&emsp;&emsp;首先说下思路，因为文献每天都有更新，所以如果要再次下新的文献，就得和已在数据库中的文献去重，所以先爬取网站上的文献标题和文献下载地址，然后和数据库中的文献标题做去重，再将这些新文献数据加载到数据库中。为什么不直接将新的数据去重写数据库呢，因为我想在数据库中保留其他的信息，比如文献abstract，而elsevier上搜索列表上没有相关文献的abstract，需要再发送一个get请求才可以获得，所以如果要更新全部的，则每个文献就发一个request，更新就会很慢。所以先做去重，再入库。 1234567891011121314151617181920#Docmanger目录结构.│├── Docmanager│ ├── __init__.py│ ├── items.py│ ├── middlewares.py│ ├── pipelines.py│ ├── settings.py│ ├── useragent.py│ ├── spiders│ │ ├── __init__.py│ │ └── Docmanager.py│ └── myextensions│ └──idle.py├── main.py├── mysql_to_redis.py├── redis_to.mysql.py├── docdownload.py└── scrapy.cfg main.py是调度整个程序执行的脚本，从输入key到爬取新的文献信息入库mysql。 123456789101112131415161718192021222324252627282930#main.pyclass Docmanager(object): def __init__(self, key, num): self.key = key self.num = num self.r = None def push_start_urls(self): self.r.flushdb() if self.num &lt;= 25: self.r.lpush('Docmanager:start_urls', "https://www.sciencedirect.com/search?qs=%s&amp;show=25&amp;sortBy=relevance&amp;offset=0" % self.key) elif self.num &lt;= 50: self.r.lpush('Docmanager:start_urls', "https://www.sciencedirect.com/search?qs=%s&amp;show=50&amp;sortBy=relevance&amp;offset=0" % self.key) else: for offset in range(0, self.num + 1 - 100, 100): self.r.lpush('Docmanager:start_urls', "https://www.sciencedirect.com/search?qs=%s&amp;show=100&amp;sortBy=relevance&amp;offset=%d" % (self.key,offset)) def main(self): self.pool = redis.ConnectionPool(host='127.0.0.1', port=6379, db=0) self.r = redis.Redis(connection_pool=self.pool) self.push_start_urls() cmdline.execute(("scrapy crawl Docmanager").split())if __name__ == '__main__': key = 'hafnium' num = 5000 with ProcessPoolExecutor(max_workers=8) as executor: executor.submit(Docmanager(key, num).main) executor(mysqltoredis(key).main) executor.map(main, [key]*3) &emsp;&emsp;key是文献的关键字，num为想要搜索的文献数量。通过push_start_urls()更具key，num将初始url加载到redis，用来redis开始爬取网页。注意到url中的how为每页文献数量，最高为100接下来我们分析需要的结构性数据。 1234567891011#items.pyclass DocItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() title = scrapy.Field() #文献标题 authors = scrapy.Field() #文献作者 journal = scrapy.Field() #文献所发表的杂志名称 date = scrapy.Field() #文献发表日期 downloadlink = scrapy.Field() #文献下载链接 link = scrapy.Field() #文献url abstractlink = scrapy.Field() #文献摘要url &emsp;&emsp;接下来，需要编写spider，因为用得是redis存储，所以用的是RedisSpider。需要在setting增加一些内容，request队列，去重，调度器以及redis的url.另外，为了更加模拟浏览器防止服务器拒绝访问，在downloadmiddleware中添加一个中间件，随机修改请求头的User-Agent。 12345#middlewares.py class UserAgentmiddleware(UserAgentMiddleware): def process_request(self, request, spider): agent = random.choice(agents) #agents 是一个列表，包含大量合法的User-Agent request.headers["User-Agent"] = agent 然后修改setting中DOWNLOADER_MIDDLEWARES即可 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#spider.pyclass DocSpider(RedisSpider): name = "Docmanager" redis_key = "Docmanager:start_urls" def __init__(self, *args, **kwargs): super(DocSpider, self).__init__(*args, **kwargs) def parse(self, response): soup = BeautifulSoup(response.text) soup = soup.find('div', class_='ResultList col-xs-24') docs_soup = soup.ol.find_all('li', recursive=False) rint(len(docs_soup)) for doc_soup in docs_soup: try: doc_soup = doc_soup.find('div', class_='result-item-container u-visited-link').find('div', class_='result-item-content', recursive=False) doc = DocItem() doc['title'] = self.gettitle(doc_soup) doc['authors'] = self.getauthors(doc_soup) doc['journal'] = doc_soup.div.find('a', class_='subtype-srctitle-link').span.string doc['link'] = response.urljoin(doc_soup.find('h2',recursive=False).a['href']) downloadlink = response.urljoin(doc_soup.find('div', class_='PreviewLinks').ol.li.span.a['href']) #有的item是书，不能直接下载， if downloadlink[-1] == 'f': doc['downloadlink'] = downloadlink #https://www.sciencedirect.com/search/api/abstract?pii=S1674987118300793 doc['abstractlink'] = self.getabstractlink(doc['downloadlink']) yield doc else: yield doc except: pass def gettitle(self, doc_soup): #获得title ls = list(doc_soup.find('h2', recursive=False).a.children) title = '' for i in ls: if isinstance(i, bs4.element.Tag): i = i.string if i: title += i return title def getauthors(self, doc_soup): #获得author ls = doc_soup.find('ol', class_=['Authors hor reduce-list','Authors hor undefined']).find_all('li', recursive=False) authors_list = [] for author in ls: authors_list.append(author.span.string) return str(authors_list) def getabstractlink(self, link): #获得文献abstract链接 pii = re.search(re.compile(r'pii/(.+?)/'), link).group(1) abstractlink = 'https://www.sciencedirect.com/search/api/abstract?pii=%s' % pii return abstractlink &emsp;&emsp;将item放入pipeline处理，这些item都是从elsevier下载到的最新的num文献数据，还没有摘要信息，首先先要和mysql中已存在的文献对比去重，所以我的想法是，将所有item中title保存在redis一个集合titles，然后根据title:item放入redis中的Hash表items。然后将mysql中的title加载到redis中的一个集合sqltitles。将title和sqltitles并集所得的集合再和sqltitles求差集就可以获得新的文献集合newtitle了。再拿newtitle去和hash表items中取得item入库。代码如下：1234567891011121314#pipeline.pyclass DocmanagerPipeline(RedisPipeline): def __init__(self, *args, **kwargs): super(DocmanagerPipeline, self).__init__(*args, **kwargs) def _process_item(self, item, spider): key0 = item['title'] key2 = 'Docmanager:titles' data1 = json.dumps(dict(item)) data2 = key0 #hash title: item self.server.hset('Docmanager:items', key0, data1) self.server.sadd(key2, data2) return item 将redis中文献集合去重本地已有文献，只留下新文献集合。&emsp;&emsp;将数据库key表中已有文献title加载到redis中集合sqltitles。mysql-to-redis.py 12345678910111213141516171819202122232425262728#mysql-to-redis.pyclass mysqltoredis(object): #通过title将mysql中items和redis中items去重 def __init__(self, key): self.key = '_'.join(key.split(' ')) def pushtoredis(self): # push the mysql title of docs to redis cur = self.conn.cursor() sql = "SELECT key_word FROM keyword_tbl WHERE key_word=?" cur.execute(sql.replace('?','%s'), [self.key]) result = cur.fetchall() if result: sql = "SELECT title From %s" % self.key cur.execute(sql) for title in cur.fetchall(): #与scrapy_redis.pineline.process_item()一致 self.r.sadd('Docmanager:sqltitles', title[0]) else: sql = "INSERT INTO keyword_tbl (key_word) VALUES (?)" cur.execute(sql.replace('?', '%s'), [self.key] or ()) self.conn.commit() def main(self): self.conn = pymysql.connect(host="localhost", port=3306, user="root", passwd="abc4494355", db="docmanager") self.pool = redis.ConnectionPool(host='127.0.0.1', port=6379, db=0) self.r = redis.Redis(connection_pool=self.pool) self.pushtoredis() &emsp;&amp;emsp。网页爬取和数据库爬取是分别在两个子进程中同时进行的，但前者比后者需要的时间慢，所以当网页爬取完毕后，就可以进行去重操作。这个操作运用了scrapy自定义扩展功能。这个扩展另外实现在没有新的request一定时间后关闭spider，避免空跑。 12345678910111213141516171819202122232425262728293031323334353637383940#myextension\idle.pyclass AutoRedisSpiderClose(object): def __init__(self, idle_number, crawler): self.idle_number = idle_number self.idle_list = [] self.idle_count = 0 self.crawler = crawler @classmethod def from_crawler(cls, crawler): if not crawler.settings.getbool('MYEXT_ENABLED'): raise NotConfigured if not 'redis_key' in crawler.spidercls.__dict__.keys(): raise NotConfigured('The type of Spider is not RedisSpider!') idle_number = crawler.settings.getint('IDLE_NUMBER', 24) ext = cls(idle_number, crawler) crawler.signals.connect(ext.spider_idle, signal=signals.spider_idle) crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed) return ext def spider_closed(self, spider): if spider.name != 'Docmanager': return pool = redis.ConnectionPool(host='127.0.0.1', port=6379, db=0) re = redis.Redis(connection_pool = pool) if re.exists('Docmanager:sqltitles'): re.sunionstore('Docmanager:titles', 'Docmanager:titles', 'Docmanager:sqltitles') re.sdiffstore('Docmanager:newtitles', 'Docmanager:titles', 'Docmanager:sqltitles') else: re.rename('Docmanager:titles','Docmanager:newtitles') def spider_idle(self, spider): if spider.name != 'Docmanager': return if not spider.server.exists(spider.redis_key): self.idle_count += 1 else: self.idle_count = 0 if self.idle_count &gt; self.idle_number: self.crawler.engine.close_spider(spider, 'Waiting %d s have no request, spider closed!' % (self.idle_number*5)) 然后修改setting中的EXTENSION和MYEXT_ENABLED。 已去重后的新文献集合(RedisDesktopManager)，从网页爬取5000条文献信息时间为1分钟。 爬取新文献详细信息并入库。&emsp;&emsp;接下来需要的就是把留下来的文献集合newtitles中的title一个一个的去items中获得item了，获得item后提取摘要链接获得摘要最后存储到mysql。&emsp;&emsp;文献量是很大的，每个文献都需要发送一个新的request，而网络IO是必须考虑到的，一定需要异步，然后就有以下选择，多进程，多线程，协程以及它们的集合。因为文献量是很大的，而多进程和多线程的都有一定上限，而进程，线程间的上下文转换不及协程，但是一个协程也只有一个线程的资源，所以我选择了多进程+协程的，用到了asyncio，aiohttp以及aiomysql。为什么不用scrapy直接异步爬取呢，其实也可以，不过又得在redis中构建一个hash表，因为爬取的摘要要和对应item对应上，一些文献摘要较长，这样redis中所占内存可能会比较大。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109redis—to-mysql.py async def create_pool(loop, **kw): #创建进程池 global __pool __pool = await aiomysql.create_pool( host=kw.get('host', '127.0.0.1'), port=kw.get('port', 3306), user=kw['user'], password=kw['password'], db=kw['db'], charset=kw.get('charset', 'UTF8MB4'), autocommit=kw.get('autocommit', True), maxsize=kw.get('maxsize', 100), minsize=kw.get('minsize', 1), loop=loop )async def connect(key): #创建一个名为关键字的新表存储信息，如果表已存在则不创建。 global __pool async with __pool.acquire() as conn: async with conn.cursor() as cur: sql = 'CREATE TABLE IF NOT EXISTS %s(' \ 'doc_id INT AUTO_INCREMENT,' \ 'title VARCHAR(255),' \ 'authors VARCHAR(255),' \ 'journal VARCHAR(255),' \ 'post_data VARCHAR(255),' \ 'abstract TEXT,' \ 'link VARCHAR(255),' \ 'downloadlink VARCHAR(255),' \ 'PRIMARY KEY (doc_id)' \ ')ENGINE=InnoDB DEFAULT CHARSET=UTF8MB4' % key await cur.execute(sql)async def push_to_mysql(r, key): #爬取摘要，并把item持久化存储 global __pool, num global stop #如果没有newtitle了 stop=True if r.exists(('Docmanager:newtitles')): title = r.spop('Docmanager:newtitles') item = json.loads(r.hget('Docmanager:items', title)) url = item['abstractlink'] if url: headers = &#123;'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36'&#125; async with aiohttp.ClientSession() as se: async with se.get(url, headers=headers) as res: abstract = await res.json() item['abstract'] = abstract['abstracts'][0]['html'] else: item['abstract'] = 'None' async with __pool.acquire() as conn: async with conn.cursor() as cur: args = [item['title'], item['authors'], item['journal'], item['abstract'], item['link'], item['downloadlink'], ] sql = 'INSERT INTO &#123;tbl&#125;' \ '(title, authors, journal, abstract, link, downloadlink)' \ 'VALUES' \ '(?, ?, ?, ?, ?, ?)'.format(tbl=key) await cur.execute(sql.replace('?', '%s'), args) num += 1 else: stop = Trueasync def task(r,key,thread_loop): #从newtitle中发布新的爬取任务，而爬取任务在子线程中。 global stop, num while not stop: asyncio.run_coroutine_threadsafe(push_to_mysql(r,key), thread_loop) await asyncio.sleep(0.05) await asyncio.sleep(15)async def init(loop,key): global stop, num stop = False num = 0 a = 0 r = redis.Redis(connection_pool=redis.ConnectionPool(host='127.0.0.1', port=6379, db=0)) while a == 0: if r.exists('Docmanager:newtitles'): loop = asyncio.get_event_loop() mysqlkw = &#123;'db': 'Docmanager', 'user': 'root', 'password': 'abc4494355'&#125; def start_loop(th_loop): #运行事件循环， loop以参数的形式传递进来运行 asyncio.set_event_loop(th_loop) th_loop.run_forever() thread_loop = asyncio.new_event_loop() t2 = threading.Thread(target=start_loop, args=(thread_loop,)) t2.daemon = True t2.start() await create_pool(loop, **mysqlkw) await connect(key) await task(r, key, thread_loop) a = 1 else: if not r.exists('Docmanager:newtitles') and not r.exists('Docmanager:titles'): break time.sleep(10)def main(key): time.sleep(10) key = '_'.join(key.split(' ')) loop = asyncio.get_event_loop() loop.run_until_complete(init(loop, key)) 这里需要提到的是，程序有两个线程运行，主线程只有一个协程，动态加载任务，而子线程中有多个协程(根据主线程创建的和已经完成的)，每个协程完成一个摘要爬取。5000新文献详细信息入库大概1分钟完成。 新的文献下载&emsp;&emsp;毕竟要创建本地文献库，那就不得不下载了，刚刚在mysql中我们已经爬取到了title和downloadlink，就可以通过downloadlink下载文献并保存到title中。 123456789101112131415161718192021222324252627282930313233#downloaddoc.pydef writer(args): doc,key = args filename = doc[0] downloadlink = doc[1] response = request.urlopen(downloadlink) link = re.search("window\\.location = '(.+?)';", bytes.decode(response.read(), encoding='utf8')).group(1) response = request.urlopen(link) if os.path.exists(r"D:\docs\%s\%s.pdf" % (key, filename)) and os.path.getsize(r"D:\docs\%s\%s.pdf" % (key, filename)) &gt; 1024: return with open(r"D:\docs\%s\%s.pdf" % (key, filename), 'wb') as f: while True: data = response.read(4096) if not data: break f.write(data) def main(key, number): #key 文献关键词 #number 文献数量 os.mkdir(r"D:\docs") if not os.path.exists(r"D:\docs") else 0 os.chdir(r"D:\docs") os.mkdir(r"D:\docs\&#123;&#125;".format(key)) if not os.path.exists(r"D:\docs\&#123;&#125;".format(key)) else 0 conn = pymysql.connect(host='127.0.0.1', port=3306, db='docmanager',user='root', passwd='abc4494355') cur = conn.cursor() sql = 'SELECT title, downloadlink FROM %s' % '_'.join(key.split(' ')) cur.execute(sql) result = cur.fetchmany(number) with ProcessPoolExecutor(max_workers=20) as executor: [executor.submit(writer, (doc, key)) for doc in result]if __name__ == '__main__': main('hafnium', 100) &emsp;&emsp;好了，将文献下载到doc中关键字文件夹内，如果下载过的则不会下载。测试100篇文献(200+MB)2分钟下完，和下载一个200MB文档消耗时间相当。 总结&emsp;&emsp;大概内容就这么多了，除了管理和下载外文文献外，其实还可以利用这些信息做下一步的分析，比如统计相关领域的关键词，进行排序后就可以得知目前该领域哪些方向比较热门，还比如统计这个领域大家一般投哪些杂志上等等。 因为request调度队列，去重都在redis，所以如果数据需求较大，可以共用一个request队列。实现分布式爬虫。 本次项目主要是以练习为主，熟悉scrapy框架，熟悉redis，mysql操作。 进一步熟悉python多进程、多线程编程，异步编程等，为以后的项目打下基础。]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目：Python开发自动预约实验软件Gettem]]></title>
    <url>%2F2018%2F07%2F03%2FGettem%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;为解决学校电镜实验预约困难影响科研进展，我通过python开发了一个可自动预约电镜实验的程序。以下介绍分为5各部分：&emsp;&emsp;1. 项目背景。&emsp;&emsp;2. 工具介绍。&emsp;&emsp;3. requests模拟登录。&emsp;&emsp;4. requests模拟发送请求。&emsp;&emsp;5. 优化程序功能，自动化设置时间，并且图形化软件。&emsp;&emsp;6. 转换成exe文件，便于操作与分享。&emsp;&emsp;7. 总结 项目背景&emsp;&emsp;透射电镜可以算是每个高校学术研究的稀缺资源，尤其在材料领域大多数研究都离不开透射电镜。本校的电镜可预约时间为每周一到周五，上午8:30-11:30和下午14:30-15:30共十个时间段.而每个时间段开放预约时间为前一周该时间段结束时间前后。这种狼多肉少的情况使得大家预约电镜非常困难，甚至有时候课题组两周都约不到一次，科研进展受到影响。所以，这个项目完成后可以很好的解决以上问题。 工具介绍Fiddler:一款开源的网络抓包工具,简单来说使fidder成为一个客户端与服务器的中间代理，客户端发送request将先发送给fidder，fidder再转发送给服务器，服务器返回的response则先到达fidder再返回客户端。 Requests:一个Python第三方HTTP库，功能全面几乎可以实现任何的http请求任务, 包括GET、OPTIONS、HEAD、POST、PUT、PATCH、DELETE。在这些request中，可根据需要的更改header，body，cookie，data等，操作方便。此外，特别值得一提的是它包含一个会话对象requests.Session()，可以跨请求保持某些参数，例如cookie，不用手动添加cookie. 在对同一个服务器连续的发出请求时，底层的TCP将被复用，就不用每发一个请求重新发送TCP连接请求，使性能显著提高。 Tkinter:一个Python的标准GUI库。使用Tkinter可以快速的创建GUI应用程序。操作简单，可以满足大多数图形化需求。 requests模拟登录&emsp;&emsp;浏览器打开网站，正常登录。通过fidder抓包。可以发现有以下request&emsp;&emsp;可以发现登录发送表单所需数据为userName，password和enter.除此之外，我们还可以发现服务器返回的是302，重定向。继续跟踪可发现还需向目标网址发送一个表单&emsp;&emsp;表单需要的数据可以发现在重定向后发送请求中的response中。&emsp;&emsp;综上，操作顺序为：发送请求（账号，密码）→在获得的重定向response中抓取信息→再发送请求→登录成功，编写代码如下123456789101112131415161718192021222324252627282930313233import request#定义会话se = request.Session()def login(userName, passWord): ''' 登录 ''' res = se.get(loginurl, timeout=15) if res.status_code == 200: form_data = &#123; "userName": userName, "passWord": passWord, "enter": True &#125; res = se.post(self.loginurl, form_data) #正则抓取toke_id token_id = search(compile('&lt;input name="tokenId" type="hidden" value="(.*?)"'), res.content.decode('utf-8')).group(1) form_data_2 = &#123; "tokenId": token_id, "account": userName, "Thirdsys": "dxsbgxxt" &#125; login_url = 'http:/xxxxxx.com' #跟进header，测试后其实不改也没关系。这里只是为了模拟浏览器更像一点。 se.update(&#123;'Referer': 'http://xxxxxx.com'&#125;) se.post(login_url, form_data_2) se.session.cookies.set('ASP.NET_SessionId',None) return#测试一下if __main__ == '__name__': userName = input('输入账号’) passWord = input('输入密码') login(userName, passWord)&emsp;&emsp;测试完成登录成功，接下来实现模拟浏览器发送表单 requests模拟发送请求&emsp;&emsp;首先分析出表单所需要的数据，通过浏览器预约一个是简单的实验。同样通过fidder抓包分析，在此过程中，我们可以通过断点来判断一共发送了哪些请求，哪些求中是必须的，哪些请求又是可以省去的，因为毕竟我们需要快速预约，所以要把过程做的最精简。通过断点测试我们发现有如下请求发送出去&emsp;&emsp;一共7个request…其中有4个post,分析这些post所需数据,分析数据来源,处理成所需数据格式，最终代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147class OrderTEM(object): ''' 物镜球差电镜预约 ''' def __init__(self, user, pwd, payload, id_, st, et, sample, project, i): ''' 定义会话，初始化表单。 params &#123; user:用户名 pwd: 密码 payload, id_为申请表单所需数据，不同用户数据不同，在注册过程中记录。 st: 预约时间段开始时间 et: 预约时间段结束时间 sample: 样品 project: 项目编号 i: 控制多线程可以均匀发表的编号，i越高，发送请求的开始时间越晚 &#125; ''' self.user = user self.pwd = pwd self.payload = payload self.id = id_ self.st = st self.et = et self.i = i self.session = requests.session() self.update = self.session.headers.update self.get = self.session.get self.post = self.session.post self.loginurl = (r'http://xxxxxx.com') self.headers1 = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36', 'Accept-Encoding': 'gzip, deflate', 'Accept-Language':'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2', 'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Upgrade-Insecure-Requests':'1', &#125; self.headers2 = &#123;'Referer': None&#125; self.session.headers.update(self.headers1) #初始化申请实验表单 self.form_data_1 = &#123; "_ajax": 1, "_object": "component_form", "_event": "submit", "cal_week_rel": '', "mode": "week", "component_id": 0, "calendar_id": 7, "name123": "仪器使用预约", "dtstart": self.st, "dtend": self.et, "project": project, "description": '', "extra_fields[12]": sample, "extra_fields[13][块体]": 'null', 'extra_fields[13][粉末]': 'on', 'extra_fields[13][有无磁性]': 'null', 'extra_fields[14][TEM]': 'on', 'extra_fields[14][HRTEM]': 'on', 'extra_fields[14][STEM]': 'null', 'extra_fields[14][STEM+mapping]': 'null', 'extra_fields[14][EDS]': 'null', 'submit': 'save' &#125; self.form_data_2 = &#123;'_ajax': 1, '_object': 'socket', '_event': 'auth', 'cal_week_rel': '', 'code' : '' &#125; def login(self): ''' 登录 ''' res = self.get(self.loginurl, timeout=15) if res.status_code == 200: form_data = &#123; "userName": self.user, "passWord": self.pwd, "enter": True &#125; res = self.post(self.loginurl, form_data) self.logincookies = self.session.cookies.get_dict() token_id = search(compile('&lt;input name="tokenId" type="hidden" value="(.*?)"'), res.content.decode('utf-8')).group(1) form_data_3 = &#123; "tokenId": token_id, "account": self.user, "Thirdsys": "dxsbgxxt" &#125; login_url = 'http://xxxxxx.com' self.update(&#123;'Referer': 'http://xxxxx.com'&#125;) self.post(login_url, form_data_3) self.session.cookies.set('ASP.NET_SessionId',None) return True else: raise '网络异常！' def perpareform(self): ''' 爬取目标页面并解析表单所需的数据 ''' url = 'http://xxxxx.com' #??self.session.headers.update(self.headers2) res1 = self.get(url) self.update(&#123;'Referer': 'http://xxxxx.com', 'Upgrade-Insecure-Requests': None&#125;) content_1 = res1.content.decode('utf-8') partern_1 = compile('"browser_[^"]+?" src="(http://xxxxx\?browser_id=(.+?)&amp;amp;st=(.+?)&amp;amp;ed=(.+?)&amp;amp;.+?form_token(.+?))"') content_1 = search(partern_1, content_1) post_url = content_1.group(1) browser_id = content_1.group(2) st2 = content_1.group(3) ed = content_1.group(4) form_token = content_1.group(5) post_url = sub(compile('amp;'),'',post_url) res2 = self.get(post_url) content_2 = res2.content.decode('utf-8') #id="calweek_5b25321017f43" partern_2 = compile('id="(calweek_.+?)"') cal_week_rel = search(partern_2,content_2).group(1) self.form_data_1['cal_week_rel' ] = cal_week_rel return post_url, browser_id, st2, ed, form_token, cal_week_rel def postform(self,post_url, browser_id, st2, ed, form_token, cal_week_rel): ''' 利用爬取到的数据填写表单并发送多个申请。 ''' res3 = self.post(post_url,self.form_data_1) self.update(&#123;'Accept': '*/*'&#125;) content_3 = res3.content.decode('utf-8') #\" id=\"uuid_5b253c614b5cd\"&gt;\ uuid = search(compile(r'(uuid_.+?)\\'),content_3).group(1) #params = &#123;'EIO': 3, 'transport': 'polling', 't': str(int(datetime.now().timestamp * 10*3)) + '-0'&#125; res4 = self.session.get('http://xxxxxx.com',params=&#123;'EIO': '3', 'transport': 'polling', 't': str(int(datetime.now().timestamp()*(10**3)))+'-0'&#125;) content_4 = res4.content sid = search(compile('"sid":"(.+?)"'),str(content_4)).group(1) self.session.get('http://xxxxxx/socket.io/',params=&#123;'EIO': '3', 'transport': 'polling', 't': str(int(datetime.now().timestamp()*(10**3)))+'-1','sid': sid&#125;) self.update(&#123;'Origin': 'http://xxxxxx.com','Content-Type': 'text/plain;charset=UTF-8'&#125;) self.post('http://xxxxxx/socket.io/',params=&#123;'EIO': '3', 'transport': 'polling', 't': str(int(datetime.now().timestamp()*(10**3)))+'-2','sid': sid&#125;,data = self.payload) self.update(&#123;'Origin': None, 'Content-Type': None&#125;) res7 = self.get('http://xxxxxx/socket.io/',params=&#123;'EIO': '3', 'transport': 'polling', 't': str(int(datetime.now().timestamp()*(10**3)))+'-3','sid': sid&#125;) content_7 = str(res7.content) code = search(compile('"code":"(.+?)"'),content_7).group(1) self.form_data_2['code'] = code res8 = self.post('http://xxxxxx.com',data = self.form_data_2) content_8 =str(res8.content) newcode = search(compile('"code":"(.+?)"'),content_8).group(1) newcode = sub(r'\\','',newcode) self.update(&#123;'Content-Type':'text/plain;charset=UTF-8'&#125;) payload2 = r'1048:42["yiqikong-reserv",&#123;"form":"&#123;\"cal_week_rel\":\"%s\",\"mode\":\"week\",\"component_id\":\"0\",\"calendar_id\":\"7\",\"name\":\"\\u4eea\\u5668\\u4f7f\\u7528\\u9884\\u7ea6\",\"dtstart\":%d,\"dtend\":%d,\"project\":\"1074\",\"description\":\"\",\"extra_fields\":&#123;\"12\":\"\",\"13\":&#123;\"\\u5757\\u4f53\":\"on\",\"\\u7c89\\u672b\":\"null\",\"\\u6709\\u65e0\\u78c1\\u6027\":\"null\"&#125;,\"14\":&#123;\"TEM\":\"on\",\"HRTEM\":\"on\",\"STEM\":\"null\",\"STEM+mapping\":\"null\",\"EDS\":\"null\"&#125;&#125;,\"submit\":\"save\",\"browser_id\":\"%s\",\"st\":\"%s\",\"ed\":\"%s\",\"equipment_id\":\"10\",\"form_token\":\"%s\",\"id\":\"7\",\"currentUserId\":\"%s\",\"SITE_ID\":\"cf\",\"LAB_ID\":\"xxx\",\"tube\":\"8fcab38c0b49b43aaf3d13888fe9444066f3940d\",\"uuid\":\"%s\"&#125;","code":"%s"&#125;]' %(cal_week_rel, self.st, self.et, browser_id, st2, ed, form_token, self.id, uuid, newcode) self.post('http://xxxxxx/socket.io/',params=&#123;'EIO': '3', 'transport': 'polling', 't': str(int(datetime.now().timestamp()*(10**3)))+'-5','sid': sid&#125;,data = payload2) self.update(&#123;'Content-Type': None&#125;) &emsp;&emsp;最开始没有设置i, 而是直接让10个线程不间断的发送请求，但是测试后发现预约成功率并不高， 分析得知该程序设计为同步发送，所以每次发送请求后必须要等到请求返回才发下一次的请求，而开放预约时间前后因用户请求暴增导致服务器返回请求缓慢，有时候达到了2s以上，这样会导致在预约时间开放之前我发送的表单还没返回的时候开放预约，而此各个线程要等到返回才发送请求，导致预约失败。于是改进程序，通过self.et - datetime.now().timestamp() - 604800 - 20 + self.i * 0.25 &lt; 0 BEGIN POST为每个线程设置发送表单的开始时间,间隔为0.25s循环发送申请。并通过记录开始发送请求到最后返回请求的时间，让线程sleep (max(2.5 - end + start,0))，使各个线程均匀间隔2.5s发送一个请求，总体上实现均匀0.25s发送一次申请，调度程序代码如下1234567891011121314151617181920212223def main_1(self): ''' 调度程序； 登录&gt; 抓取必要元素并填写表单 &gt; 提交表单 ''' self.login() sleep(10) post_url, browser_id, st2, ed, form_token, cal_week_rel = self.perpareform() while 1: while self.et - datetime.now().timestamp() - 604800 - 20 + self.i * 0.25 &lt; 0: while 1: try: start = datetime.now().timestamp() self.postform(post_url, browser_id, st2, ed, form_token, cal_week_rel) if self.et - datetime.now().timestamp() - 604800 + 10 &lt; 0: break end = datetime.now().timestamp() sleep (max(2.5 - end + start,0)) except: pass break if self.et - datetime.now().timestamp() - 604800 + 10 &lt; 0: break 优化程序功能; 自动计算预约时间；注册功能；图形化设计自动计算预约时间&emsp;&emsp;因为预约开放时间是固定的，所以每次欲预约的实验时间是可以预算的，即下周上午或者下午，若在11:30之前，预约时间为下周8:30-11:30，超过11:30直到下午5:30，即预约下周下午时间段。所以可以设置一个默认时间，而自定义时间是因为有些同学经过培训可以预约中午或者晚上。添加一个getst模块。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class getst(object): def __init__(self, opt, equipment = None, ymd = None, date2 = None ): ''' opt:用户选择自定义时间还是默认, equipment:选择设备，因为设备不同中午时间有所 ymd, date2: 用户自定义选择的时间。 ''' self.opt = opt self.ymd = ymd self.date2 = date2 self.equipment = equipment def default(self): ''' 默认时间，根据当前时间计算出最近即将开放的时间段。 ''' today = datetime.now() t = (today.hour,today.minute) if t &lt; (11,31): st = datetime(today.year, today.month, today.day, 8, 30, 0 ).timestamp() + 604800 elif t &lt; (17,31): st = datetime(today.year, today.month, today.day, 14, 30, 0 ).timestamp() + 604800 else: st = datetime(today.year, today.month, today.day, 8, 30, 0 ).timestamp() + 691200 return int(st) def userdefined(self, ymd, date2, equipment): ''' 自定义预约时间 ''' #ymd is dict,&#123;'year':year,....&#125; if date2 == '早': st = int(datetime(ymd['year'], ymd['month'], ymd['day'], 8, 30, 0).timestamp()) elif date2 == '下午': st = int(datetime(ymd['year'], ymd['month'], ymd['day'], 14, 30, 0).timestamp()) elif date2 == '晚上': st = int(datetime(ymd['year'], ymd['month'], ymd['day'], 19, 0, 0).timestamp()) else: if equipment == '1': st = int(datetime(ymd['year'], ymd['month'], ymd['day'], 12, 0, 0).timestamp()) else: st = int(datetime(ymd['year'], ymd['month'], ymd['day'], 11, 45, 0).timestamp()) return int(st) def getst(self): ''' return: start time 根据用户选择默认或自定义 默认则为下一次可预约时间 自定义则约抢任意时间，包括针对管理员的中午时间段和晚上时间段。 ''' if self.opt == 'userdefind': return self.userdefined(self.ymd, self.date2, self.equipment) else: return self.default() 多个实验预约整合在一起; 多线程&emsp;&emsp; 以上已经完成了模拟登录，模拟发送表单，自动获得预约时间，下面将整合输入，整合所有实验模块，并实现多线程。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667def gettem(name, equipment, sample, project, time, log_, date = None, date2 = None): ''' name: 用户名 equipment: 目标设备 sample: 样品名 project: 项目号码 time: 时间选择(默认，自定义) log_: 日志 date: 自定义日期 date2: 自定义时间段 ''' #------------------------------------------------------------------------# if time =='userdefind': st = getst('userdefind', equipment, date, date2).getst() if date2 != '中': et = st + 10799 else: et = st + 8999 else: st = getst('default').getst() et = st + 10799 if equipment == 1: log_.insert('end','准备预约：球差\n' ) else: log_.insert('end','准备预约：场发射\n' ) log_.insert('end','%s\n' % (datetime.fromtimestamp(st).strftime('%a %Y-%m-%d %H:%M:%S'))) log_.insert('end','%s\n' % (datetime.fromtimestamp(et).strftime(' %Y-%m-%d %H:%M:%S'))) while 1: c = datetime.now().timestamp() #设置子线程开始登录时间为 -40s if et - 604800 - 40 &lt; c: with open('users.txt','r') as f: userdicts = json.loads(f.read()) userdict = userdicts[name] if equipment == 1: userdict = userdicts['%s' % name] log_.insert('end','开始登录...\n') for i in range(10): a = OrderTEM_1(userdict['username'],userdict['pwd'], userdict['payload'], userdict['id'], st, et, sample, project, i) p = Thread(target = a.main_1) p.daemon = True p.start() sleep(15) log_.insert('end','开始交表...\n') elif equipment == 2: userdict = userdicts['%s' % name] log_.insert('end','开始登录...、\n') for j in range(10): b = OrderTEM_2(userdict['username'],userdict['pwd'], userdict['payload'], userdict['id'], st, et, sample, project, j) q = Thread(target = b.main_2) q.daemon = True q.start() sleep(15) log_.insert('end','开始交表...\n') sleep(35) checkit = check(userdict['username'], userdict['pwd'] , st, et , name, equipment) checkit.login_check() form_data = checkit.perpareform_check() dict_ = checkit.checking(form_data) content = dict_['content'] name = search(checkit.username, content) if name: log_.insert('end','预约成功') else: log_.insert('end','预约失败') break sleep(10)&emsp;&emsp;这里面有个多出来的check模块，它的功能是可以检测本次预约是否成功并返回结果，是根据本次目标时间爬取网站上该时间段预约人的名字与用户名对比判断的，比较简单，就不赘述了。 图形化设计&emsp;&emsp;为了方便程序，图形化软件。整合输入参数，除账号之外，预约实验需要添加的信息有时间选择(自定义|默认)，实验选择，样品，项目编号等。通过python自带的Tkinter库简易实现图形化，编写代码如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200def gui(): ''' Getten图形化 ''' def run(): ''' 开始运行 ''' tem = vartem.get() time = vartime.get() name = ename.get().strip() account = eaccont.get() sample = esample.get() date = edate.get().strip() date2 = comdate2.get() project = comproject.get() if tem and time and name and account and sample and project: if time == 'userdefind': if len(date) == 8: date = &#123;'year':int(date[:4]),'month':int(date[4:6]),'day':int(date[6:8])&#125; else: tk.messagebox.showerror('错误', '自定义时间有误?') with open('users.txt', 'r') as f: userdicts = json.loads(f.read()) if name in userdicts: if account != userdicts[name]['username']: tk.messagebox.showerror('错误','用户名与账号不匹配') else: th = Thread(target = gettem, args=(name, tem, sample, project, time, log_, date, date2)) th.daemon = True th.start() runbutton.config(state = 'disabled') ename.config(state = 'disabled') eaccont.config(state = 'disabled') esample.config(state = 'disabled') edate.config(state = 'disabled') comdate2.config(state = 'disabled') comproject.config(state = 'disabled') rtem1.config(state = 'disabled') rtem2.config(state = 'disabled') rtime1.config(state = 'disabled') rtime2.config(state = 'disabled') signbutton.config(state = 'disabled') else: t = tk.messagebox.askokcancel('注册','你还没注册,请先注册吧') if t == True: sign() else: tk.messagebox.showerror('错误','有信息未填，请检查。') def quit(): ''' 关闭 ''' os._exit(0) def datef(): edate.config(state = 'disable') comdate2.config(state = 'disable') def datef2(): edate.config(state = 'normal') comdate2.config(state = 'readonly') window = tk.Tk() window.title('Gettem') window.geometry('200x500+600+150') window.resizable(width = False, height = False) window.protocol("WM_DELETE_WINDOW", quit) #框架，图片 mainframe = tk.Frame(window, bg = 'White') mainframe.pack(fill = 'both', expand = 1) #labels linfo = tk.Label(mainframe, text = '使用方法\n1.注册，将姓名及账号进行登记。\n2.注册完成后,选择时间。\n (默认为下次开放时间)\n\3.填写信息，点击开始。\n (至少提前30s开始)\n4.保持网络，不关机。\n5.组内同学使用,切勿外传。', justify = 'left', bg = 'White', wraplength = 240,font = ('黑体',8)) ltem = tk.Label(text='TEM：',font=('黑体',10), fg = 'DeepSkyBlue', bg = 'White') ltime = tk.Label(text = '时间：',font=('黑体',10), fg = 'DeepSkyBlue', bg = 'White') lname = tk.Label(text = '名字：', font=('黑体',10), fg = 'DeepSkyBlue', bg = 'White') laccont = tk.Label(text = '账号：', font=('黑体',10), fg = 'DeepSkyBlue', bg = 'White') lsample = tk.Label(text = '样品：', font=('黑体',10), fg = 'DeepSkyBlue', bg = 'White') ldate = tk.Label(text = '日期：', font=('黑体',10), fg = 'DeepSkyBlue', bg = 'White') lparoject = tk.Label(text = '项目：', font=('黑体',10), fg = 'DeepSkyBlue', bg = 'White') linfo.place(x = 6, y = 2) ltem.place(x = 8, y = 110) ltime.place(x = 8, y = 145) lname.place(x = 8, y = 180) laccont.place(x = 8, y = 215) lsample.place(x = 8, y = 250) ldate.place(x = 8, y = 285) lparoject.place(x = 8, y = 320) #选择按钮 vartem = tk.IntVar() vartime = tk.StringVar() vartem.set(1) vartime.set('default') rtem1 = tk.Radiobutton(text = '球差', bg = 'White', variable = vartem, value = 1) rtem2 = tk.Radiobutton(text = '场发射', bg = 'White', variable = vartem, value = 2) rtime1 = tk.Radiobutton(text = '默认', bg = 'White', variable = vartime, value = 'default', command = datef) rtime2 = tk.Radiobutton(text = '自定义', bg = 'White', variable = vartime, value = 'userdefind', command = datef2) rtem1.place(x = 60, y = 107) rtem2.place(x = 115, y = 107) rtime1.place(x = 60, y = 142) rtime2.place(x = 115, y = 142) vardate = tk.StringVar() vardate.set((datetime.now() + timedelta(days=7)).strftime('%Y%m%d')) #输入entry ename = tk.Entry(bg = 'Lightyellow', width = 15, font = ('黑体',11), highlightcolor = 'DeepSkyBlue', justify = 'center',highlightthickness = 1) eaccont = tk.Entry(bg = 'Lightyellow', width = 15, font = ('黑体',11), highlightcolor = 'DeepSkyBlue', justify = 'center',highlightthickness = 1) esample = tk.Entry(bg = 'Lightyellow', width = 15, font = ('黑体',11), highlightcolor = 'DeepSkyBlue', justify = 'center',highlightthickness = 1) edate = tk.Entry(bg = 'Lightyellow', width = 12, font = ('黑体',8), highlightcolor = 'DeepSkyBlue', justify = 'center',highlightthickness = 1, textvariable = vardate, state = 'disable') ename.place(x = 60, y = 180, heigh = 25) eaccont.place(x = 60, y = 215, heigh = 25) esample.place(x = 60, y = 250, heigh = 25) edate.place(x = 60, y = 285, heigh = 25) #下拉菜单：project 和 早中晚 vardate2 = StringVar() comdate2 = ttk.Combobox(mainframe, width = 4, justify = 'center', font = ('宋体',8), textvariable = vardate2, state = 'disable') comdate2['value'] = ('早', '中', '下午', '晚上') comdate2.place(x = 140, y = 286, heigh = 23) comdate2.current(1) varproject = IntVar() comproject = ttk.Combobox(mainframe, width = 17, justify = 'center', font = ('宋体',8), textvariable = varproject) comproject['value'] = (1074, 1084, 1180) comproject.place(x = 60, y = 319, heigh = 23) comproject.current(0) #buttons runbutton = tk.Button(width = 8, height = 1, text = '开始', font = ('黑体', 11), bg = 'Aliceblue', fg = 'DeepSkyBlue', relief = 'ridge', bd = 2, cursor = 'target', command = run) signbutton = tk.Button(text = '注册', font = ('黑体', 9), bg = 'White', fg = 'Red', bd = 0, cursor = 'target', command = sign) runbutton.place(x = 60, y = 350) signbutton.place(x = 150, y = 360) log_ = scrolledtext.ScrolledText(width = 250, height = 10, font = ('宋体', 10, ' bold'), relief = 'sunken', bg = 'Lightyellow', wrap=tk.WORD) log_.place(y = 390) window.mainloop()def sign(): ''' 账号注册 ''' def user_sign_check(event): sign_user_pwd = enew_pwd.get() sign_user_name = enew_username.get() sign_user_account = enew_account.get() if not (sign_user_pwd and sign_user_name and sign_user_account): tk.messagebox.showerror('错误', '请完整填写信息') else: try: with open('users.txt', 'r') as f: userdicts = json.loads(f.read()) except FileNotFoundError: with open('users.txt', 'w') as f: userdicts = &#123;'name': &#123;&#125;&#125; t = json.dumps(userdicts) f.write(t) if sign_user_name in userdicts: tk.messagebox.showerror('错误', '你已经注册过了...') else: st = getst('default').default() et = st + 10799 #b = OrderTEM_2(userdict['username'],userdict['pwd'], userdict['payload'], userdict['id'], st, et, sample, project, j) check_ = OrderTEM_1(sign_user_account, sign_user_pwd, '', '', st, et,'hf' , 1000, 0) t = check_.login_sign() if t == -2: tk.messagebox.showerror('错误','网络异常，请检查') elif t == -1: tk.messagebox.showerror('错误', '账号不存在或密码错误') else: id_ = t post_url, browser_id, st2, ed, form_token, cal_week_rel = check_.perpareform_1() payload = check_.get_payload(post_url, browser_id, st2, ed, form_token, cal_week_rel) userdicts[sign_user_name] = &#123;'username':sign_user_account, 'pwd':sign_user_pwd, 'payload':payload, 'id':id_&#125; with open('users.txt', 'w') as f: t = json.dumps(userdicts) f.write(t) tk.messagebox.showinfo('OK', '注册成功！') win_sign.destroy() win_sign = tk.Toplevel() win_sign.title('注册') win_sign.geometry('250x200+650+300') mainframe = tk.Frame(win_sign, bg = 'White') mainframe.pack(fill='both' ,expand=1) lname = tk.Label(win_sign,text='*姓名：',font=('黑体',10), fg = 'DeepSkyBlue', bg = 'White') laccount = tk.Label(win_sign, text = '*账号：',font=('黑体',10), fg = 'DeepSkyBlue', bg = 'White') lpwd = tk.Label(win_sign, text = '*密码：', font=('黑体',10), fg = 'DeepSkyBlue', bg = 'White') lname.place(x = 10, y = 20) laccount.place(x = 10, y = 60) lpwd.place(x = 10, y = 100) enew_username = tk.Entry(win_sign, bg = 'Lightyellow', width = 20, font = ('黑体',11), highlightcolor = 'DeepSkyBlue', justify = 'center',highlightthickness = 1) enew_account = tk.Entry(win_sign, bg = 'Lightyellow', width = 20, font = ('黑体',11), highlightcolor = 'DeepSkyBlue', justify = 'center',highlightthickness = 1) enew_pwd = tk.Entry(win_sign, bg = 'Lightyellow', width = 20, font = ('黑体',11), highlightcolor = 'DeepSkyBlue', justify = 'center',highlightthickness = 1,show = '*') enew_username.place(x = 60, y = 20, heigh = 25) enew_account.place(x = 60, y = 60, heigh = 25) enew_pwd.place(x = 60, y = 100, heigh = 25) signbutton = tk.Button(win_sign, text='注册',font=('黑体',14), fg = 'DeepSkyBlue', bg = 'White', relief = 'ridge', bd = 0, command = (lambda : user_sign_check(1))) signbutton.place(x = 100, y = 140) enew_username.bind("&lt;Return&gt;", user_sign_check) enew_pwd.bind("&lt;Return&gt;", user_sign_check) enew_account.bind("&lt;Return&gt;", user_sign_check)&emsp;&emsp;考虑到程序安全性，可以先把交给课题组某个同学负责，然后其他同学都可以让负责的同学帮忙预约实验，而添加注册功能就可以同学第一次先注册后，以后则不用告诉负责同学密码，只需输入姓名和账户名即可，这样不仅保护了同学密码，也保护了软件。&emsp;&emsp;主要界面&emsp;&emsp;注册界面&emsp;&emsp;开始运行 转换成exe文件，便于操作与分享。&emsp;&emsp;通过pyInstaller将py转换成exe，需要安装pywin32库。在命令行中输入python pyinstaller.py -F Gettem.py。 总结&emsp;&emsp;好了，基本工作就到此就基本完成了。接下来就是测试这个程序好不好使，至少到目前为止，共预约5次，每次都成功预约，成功率达到100%。如果以后要改进程序的话，可以从以下几个方面考虑： 增加线程数，经测试，增加线程可以一定程度上提高提交频率，但有上限值，因为网络瓶颈。 将同步改成异步，使得每次发送请求后不用等到返回，在一定时间内继续发送，这样不仅可以提高提交频率并且可以使发送请求的时间间隔更加均匀。但这将要大刀阔斧更改程序，还需要把多线程改为多进程，需要一定工作量。 将多线程转换成多进程，因为python的GIL锁缘故，多线程并不是真正意义上的多线程，只是并发了，那为什么这里要设置多线程呢，因为在一个线程发送请求时还没返回时这是一个I/O操作，这个等待的时间就会让其他线程去发送请求了，所以还是可以提高发送请求速度。但CPU还是只用一个CPU，而如果用多进程的话，就可以更大利用资源，不过Gettem需要CPU资源实在很少，所以多进程的作用不太理想。]]></content>
      <categories>
        <category>项目</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F05%2F17%2Fpython%2F</url>
    <content type="text"><![CDATA[Hello World！]]></content>
  </entry>
</search>
